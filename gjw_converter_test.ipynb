{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening datastore /Users/GJWood/nilm_gjw_data\\HDF5\\nilm_gjw_data.hdf5\n",
      "checking c:\\Users\\GJWood\\nilm_gjw_data\n",
      "checking c:\\Users\\GJWood\\nilm_gjw_data\\building1\n",
      "checking c:\\Users\\GJWood\\nilm_gjw_data\\building1\\elec\n",
      "found files for date: 2013-11-20 ['2013-10-17T18:19:07.000000000+0100'] to ['2013-11-20T14:26:53.000000000+0000']\n",
      "found files for date: 2013-11-28 ['2013-10-21T20:30:26.000000000+0100'] to ['2013-11-28T11:03:40.000000000+0000']\n",
      "found files for date: 2013-12-01 ['2013-10-24T00:12:05.000000000+0100'] to ['2013-12-01T20:06:23.000000000+0000']\n",
      "found files for date: 2013-12-04 ['2013-11-08T12:16:21.000000000+0000'] to ['2013-12-04T15:33:05.000000000+0000']\n",
      "found files for date: 2013-12-13 ['2013-11-16T16:14:40.000000000+0000'] to ['2013-12-13T11:57:52.000000000+0000']\n",
      "found files for date: 2013-12-27 ['2013-12-02T13:15:47.000000000+0000'] to ['2013-12-27T20:36:46.000000000+0000']\n",
      "found files for date: 2014-01-09 ['2013-12-15T22:15:33.000000000+0000'] to ['2014-01-09T16:04:49.000000000+0000']\n",
      "found files for date: 2014-02-09 ['2014-01-12T13:02:22.000000000+0000'] to ['2014-02-09T13:29:49.000000000+0000']\n",
      "found files for date: 2014-03-27 ['2014-03-13T21:59:56.000000000+0000'] to ['2014-03-26T11:19:40.000000000+0000']\n",
      "found files for date: 2014-05-26 ['2014-04-27T00:17:57.000000000+0100'] to ['2014-05-26T20:19:31.000000000+0100']\n",
      "found files for date: 2015-05-12 ['2015-04-17T12:48:40.000000000+0100'] to ['2015-05-12T15:52:28.000000000+0100']\n",
      "found files for date: 2015-05-27 ['2015-05-03T05:44:28.000000000+0100'] to ['2015-05-27T15:26:35.000000000+0100']\n",
      "found files for date: 2015-06-03 ['2015-05-10T13:48:52.000000000+0100'] to ['2015-06-03T10:04:05.000000000+0100']\n",
      "physical_quantity          power         \n",
      "type                      active reactive\n",
      "2013-10-17 18:19:07+01:00    382        0\n",
      "2013-10-17 18:19:08+01:00    449        0\n",
      "2013-10-17 18:19:09+01:00    449        0\n",
      "2013-10-17 18:19:10+01:00    449        0\n",
      "... 17387397 rows at Prepared for tool kit\n",
      "physical_quantity          power         \n",
      "type                      active reactive\n",
      "2015-06-03 10:04:02+01:00    984        0\n",
      "2015-06-03 10:04:03+01:00    984        0\n",
      "2015-06-03 10:04:04+01:00    984        0\n",
      "2015-06-03 10:04:05+01:00    732        0\n",
      "Done converting YAML metadata to HDF5!\n",
      "Done converting gjw to HDF5!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from os.path import join, isdir, isfile\n",
    "from os import listdir\n",
    "import fnmatch\n",
    "import re\n",
    "from sys import stdout\n",
    "from nilmtk.utils import get_datastore\n",
    "from nilmtk.datastore import Key\n",
    "from nilmtk.timeframe import TimeFrame\n",
    "from nilmtk.measurement import LEVEL_NAMES\n",
    "from nilmtk.utils import get_module_directory, check_directory_exists\n",
    "from nilm_metadata import convert_yaml_to_hdf5, save_yaml_to_datastore\n",
    "\n",
    "column_mapping = {\n",
    "    'frequency': ('frequency', \"\"),\n",
    "    'voltage': ('voltage', \"\"),\n",
    "    'W': ('power', 'active'),\n",
    "    'active': ('power', 'active'),\n",
    "    'energy': ('energy', 'apparent'),\n",
    "    'A': ('current', ''),\n",
    "    'reactive_power': ('power', 'reactive'),\n",
    "    'apparent_power': ('power', 'apparent'),\n",
    "    'power_factor': ('pf', ''),\n",
    "    'PF': ('pf', ''),\n",
    "    'phase_angle': ('phi', ''),\n",
    "    'VA': ('power', 'apparent'),\n",
    "    'VAR': ('power', 'reactive'),\n",
    "    'reactive': ('power', 'reactive'),\n",
    "    'VLN': ('voltage', \"\"),\n",
    "    'V': ('voltage', \"\"),\n",
    "    'f': ('frequency', \"\")\n",
    "    \n",
    "}\n",
    "# data for file name manipulation\n",
    "TYPE_A = \"active\"\n",
    "TYPE_R = \"reactive\"\n",
    "\n",
    "filename_prefix_mapping = {\n",
    "    TYPE_A : ('4-POWER_REAL_FINE '),\n",
    "    TYPE_R : ('5-POWER_REACTIVE_STANDARD ')\n",
    "}\n",
    "filename_suffix_mapping = {\n",
    "    TYPE_A : (' Dump'),\n",
    "    TYPE_R : (' Dump')\n",
    "}\n",
    "\n",
    "# DataFrame column names\n",
    "TIMESTAMP_COLUMN_NAME = \"timestamp\"\n",
    "ACTIVE_COLUMN_NAME = \"active\"\n",
    "REACTIVE_COLUMN_NAME = \"reactive\"\n",
    "\n",
    "type_column_mapping = {\n",
    "    TYPE_A : (ACTIVE_COLUMN_NAME),\n",
    "    TYPE_R : (REACTIVE_COLUMN_NAME) \n",
    "}\n",
    "\n",
    "\n",
    "TIMEZONE = \"Europe/London\" # local time zone\n",
    "home_dir='/Users/GJWood/nilm_gjw_data' # path to input data\n",
    "\n",
    "#regular expression matching\n",
    "bld_re = re.compile('building\\d+') #used to pull building name from directory path\n",
    "bld_nbr_re = re.compile ('\\d+') # used to pull the building number from the name\n",
    "iso_date_re = re.compile ('\\d{4}-\\d{2}-\\d{2}') # used to pull the date from the file name\n",
    "\n",
    "def convert_gjw(gjw_path, output_filename, format=\"HDF\"):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    gjw_path : str\n",
    "        The root path of the gjw dataset.\n",
    "    output_filename : str\n",
    "        The destination filename (including path and suffix), will default if not specified\n",
    "    directory and file structure\n",
    "    nilm_gjw_data\n",
    "        building<1>\n",
    "            elec\n",
    "                4-POWER_REAL_FINE <date> Dump.csv\n",
    "                5-POWER_REACTIVE_STANDARD <date> Dump.csv\n",
    "                ...\n",
    "        ...\n",
    "        building<n>\n",
    "        HDF5\n",
    "            nilm_gjw_data.hdf5\n",
    "        metadata\n",
    "            building1.yaml\n",
    "            dataset.yaml\n",
    "            meter_devices.yaml\n",
    "        other files    \n",
    "    \"\"\"\n",
    "    if gjw_path is None: gjwpath = home_dir\n",
    "    check_directory_exists(gjw_path)\n",
    "    os.chdir(gjw_path)\n",
    "    gjw_path = os.getcwd()  # sort out potential issue with slashes or backslashes\n",
    "    if output_filename is None:\n",
    "        output_filename =join(home_dir,'HDF5','nilm_gjw_data.hdf5')\n",
    "    elec_path = join(gjw_path, 'building1','elec')\n",
    "    # Open data store\n",
    "    print( 'opening datastore', output_filename)\n",
    "    store = get_datastore(output_filename, format, mode='w')\n",
    "    # walk the directory tree from the dataset home directory\n",
    "    #clear dataframe & add column headers\n",
    "    df = pd.DataFrame(columns=[ACTIVE_COLUMN_NAME,REACTIVE_COLUMN_NAME])\n",
    "    found = False\n",
    "    for current_dir, dirs_in_current_dir, files in os.walk(gjw_path):\n",
    "        if current_dir.find('.git')!=-1 or current_dir.find('.ipynb') != -1:\n",
    "            #print( 'Skipping ', current_dir)\n",
    "            continue\n",
    "        print( 'checking', current_dir)\n",
    "        m = bld_re.search(current_dir)\n",
    "        if m: #The csv files may be further down the tree so this section may be repeated\n",
    "            building_name = m.group()\n",
    "            building_nbr = int(bld_nbr_re.search(building_name).group())\n",
    "            meter_nbr = 1\n",
    "            key = Key(building=building_nbr, meter=meter_nbr)\n",
    "        for items in fnmatch.filter(files, \"4*.csv\"):\n",
    "            # process any .CSV files found\n",
    "            found = True\n",
    "            ds = iso_date_re.search(items).group()\n",
    "            print( 'found files for date:', ds,end=\" \")\n",
    "            # found files to process\n",
    "            df1 = _read_file_pair(current_dir,ds) # read two csv files into a dataframe    \n",
    "            df = pd.concat([df,df1]) # concatenate the results into one long dataframe\n",
    "        if found:\n",
    "            found = False\n",
    "            df = _prepare_data_for_toolkit(df)\n",
    "            _summarise_dataframe(df,'Prepared for tool kit')\n",
    "            store.put(str(key), df)\n",
    "            #clear dataframe & add column headers\n",
    "            #df = pd.DataFrame(columns=[ACTIVE_COLUMN_NAME,REACTIVE_COLUMN_NAME])\n",
    "            break # only 1 folder with .csv files at present\n",
    "    store.close()\n",
    "    convert_yaml_to_hdf5(join(gjw_path, 'metadata'),output_filename)\n",
    "    print(\"Done converting gjw to HDF5!\")\n",
    "\n",
    "def _read_and_standardise_file(dir,ds,type):   \n",
    "    \"\"\"\n",
    "    parameters \n",
    "        dir  - the directory path where the files may be found\n",
    "        ds   - the date string which identifies the pair of files\n",
    "        type - the type of data to be read\n",
    "    The filename is constructed using the appropriate prefixes and suffixes\n",
    "    The data is then read, merged, de-duplicated, converted to the correct time zone\n",
    "    and converted to a time series and resampled per second\n",
    "    \"\"\"\n",
    "    fn = filename_prefix_mapping[type]+ds+filename_suffix_mapping[type]+'.csv'\n",
    "    ffn = join(dir,fn)\n",
    "    df = pd.read_csv(ffn,names=[TIMESTAMP_COLUMN_NAME,type_column_mapping[type]])\n",
    "    df.drop_duplicates(subset=[TIMESTAMP_COLUMN_NAME], inplace=True) # remove duplicate rows with same timestamp\n",
    "    df.index = pd.to_datetime(df.timestamp.values, unit='s', utc=True) # convert the index to time based\n",
    "    df = df.tz_convert(TIMEZONE) #deal with summertime etc. for London timezone\n",
    "    # re-sample on single file only as there may be gaps between dumps            \n",
    "    df = df.resample('S',fill_method='ffill') # make sure we have a reading for every second\n",
    "    # resample seems to remove the timestamp column so put it back\n",
    "    df[TIMESTAMP_COLUMN_NAME] = df.index\n",
    "    df.drop_duplicates(subset=TIMESTAMP_COLUMN_NAME, inplace=True)\n",
    "    return df\n",
    "\n",
    "def _read_file_pair(dir,ds):\n",
    "    \"\"\"\"\n",
    "    parameters \n",
    "        dir - the directory path where the files may be found\n",
    "        ds  - the date string which identifies the pair of files\n",
    "    The files are processed individually then the columns merged on matching timestamps   \n",
    "    \"\"\"\n",
    "    df1 = _read_and_standardise_file(dir,ds,TYPE_A)\n",
    "    #_summarise_dataframe(df1,'read file: '+TYPE_A)\n",
    "    df2 = _read_and_standardise_file(dir,ds,TYPE_R)\n",
    "    #_summarise_dataframe(df2,'read file: '+TYPE_R)  \n",
    "    df3 = pd.merge(df1,df2,on=TIMESTAMP_COLUMN_NAME, how='outer') #merge the two column types into 1 frame\n",
    "    df3.fillna(value=0, inplace=True) # may need to enter initial entries to reactive sequence\n",
    "    #_summarise_dataframe(df3,'return from merge and fillna)\n",
    "    print(df3[TIMESTAMP_COLUMN_NAME].head(1).values,\"to\",df3[TIMESTAMP_COLUMN_NAME].tail(1).values) #print first and last entries\n",
    "    return df3\n",
    "\n",
    "def _prepare_data_for_toolkit(df):\n",
    "    #remove any duplicate timestamps between files\n",
    "    df.drop_duplicates(subset=[\"timestamp\"], inplace=True) # remove duplicate rows with same timestamp\n",
    "    df.index = pd.to_datetime(df.timestamp.values, unit='s', utc=True) # convert the index to time based\n",
    "    df = df.tz_convert(TIMEZONE) #deal with summertime etc. for London timezone\n",
    "    df = df.drop(TIMESTAMP_COLUMN_NAME,1) # remove the timestamp column  \n",
    "    df.rename(columns=lambda x: column_mapping[x], inplace=True) # Renaming from gjw header to nilmtk controlled vocabulary\n",
    "    df.columns.set_names(LEVEL_NAMES, inplace=True) # Needed for column levelling (all converter need this line)\n",
    "    df = df.convert_objects(convert_numeric=True) # make sure everything is numeric\n",
    "    df = df.dropna() # drop rows with empty cells\n",
    "    df = df.astype(np.float32) # Change float 64 (default) to float 32 \n",
    "    df = df.sort_index() # Ensure that time series index is sorted\n",
    "    return df\n",
    "\n",
    "def _summarise_dataframe(df,loc):\n",
    "    print(df.head(4))\n",
    "    print(\"...\", len(df.index),\"rows at\", loc)\n",
    "    print (df.tail(4))\n",
    "    \n",
    "def main():\n",
    "    convert_gjw('c:/Users/GJWood/nilm_gjw_data', None)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
