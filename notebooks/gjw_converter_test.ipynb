{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening datastore /Users/GJWood/nilm_gjw_data\\HDF5\\nilm_gjw_data.hdf5\n",
      "checking c:\\Users\\GJWood\\nilm_gjw_data\n",
      "checking c:\\Users\\GJWood\\nilm_gjw_data\\building1\n",
      "checking c:\\Users\\GJWood\\nilm_gjw_data\\building1\\elec\n",
      "found files for date: 2013-11-20 0   2013-10-17 17:19:07\n",
      "Name: timestamp, dtype: datetime64[ns] to 2927266   2013-11-20 14:26:53\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2013-11-28 0   2013-10-21 19:30:26\n",
      "Name: timestamp, dtype: datetime64[ns] to 3252794   2013-11-28 11:03:40\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2013-12-01 0   2013-10-23 23:12:05\n",
      "Name: timestamp, dtype: datetime64[ns] to 3358458   2013-12-01 20:06:23\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2013-12-04 0   2013-11-08 12:16:21\n",
      "Name: timestamp, dtype: datetime64[ns] to 2258204   2013-12-04 15:33:05\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2013-12-13 0   2013-11-16 16:14:40\n",
      "Name: timestamp, dtype: datetime64[ns] to 2317392   2013-12-13 11:57:52\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2013-12-27 0   2013-12-02 13:15:47\n",
      "Name: timestamp, dtype: datetime64[ns] to 2186459   2013-12-27 20:36:46\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2014-01-09 0   2013-12-15 22:15:33\n",
      "Name: timestamp, dtype: datetime64[ns] to 2137756   2014-01-09 16:04:49\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2014-02-09 0   2014-01-12 13:02:22\n",
      "Name: timestamp, dtype: datetime64[ns] to 2420847   2014-02-09 13:29:49\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2014-03-27 0   2014-03-13 21:59:56\n",
      "Name: timestamp, dtype: datetime64[ns] to 1084784   2014-03-26 11:19:40\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2014-05-26 0   2014-04-26 23:17:57\n",
      "Name: timestamp, dtype: datetime64[ns] to 2577694   2014-05-26 19:19:31\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2015-05-12 0   2015-04-17 11:48:40\n",
      "Name: timestamp, dtype: datetime64[ns] to 2171028   2015-05-12 14:52:28\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2015-05-27 0   2015-05-03 04:44:28\n",
      "Name: timestamp, dtype: datetime64[ns] to 2108527   2015-05-27 14:26:35\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2015-06-03 0   2015-05-10 12:48:52\n",
      "Name: timestamp, dtype: datetime64[ns] to 2060113   2015-06-03 09:04:05\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "found files for date: 2015-06-10 0   2015-05-17 04:39:18\n",
      "Name: timestamp, dtype: datetime64[ns] to 2116918   2015-06-10 16:41:16\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "physical_quantity            power         \n",
      "type                      apparent reactive\n",
      "2013-10-17 18:19:07+01:00      382        0\n",
      "2013-10-17 18:19:08+01:00      449        0\n",
      "2013-10-17 18:19:09+01:00      449        0\n",
      "2013-10-17 18:19:10+01:00      449        0\n",
      "... 18019628 rows at Prepared for tool kit\n",
      "physical_quantity            power         \n",
      "type                      apparent reactive\n",
      "2015-06-10 17:41:13+01:00      682        0\n",
      "2015-06-10 17:41:14+01:00      654        0\n",
      "2015-06-10 17:41:15+01:00      654        0\n",
      "2015-06-10 17:41:16+01:00      631        0\n",
      "Done converting YAML metadata to HDF5!\n",
      "Done converting gjw to HDF5!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join, isfile\n",
    "import fnmatch\n",
    "import re\n",
    "from nilmtk.utils import get_datastore\n",
    "from nilmtk.datastore import Key\n",
    "from nilmtk.measurement import LEVEL_NAMES\n",
    "from nilmtk.utils import check_directory_exists\n",
    "from nilm_metadata import convert_yaml_to_hdf5, save_yaml_to_datastore\n",
    "\n",
    "column_mapping = {\n",
    "    'frequency': ('frequency', \"\"),\n",
    "    'voltage': ('voltage', \"\"),\n",
    "    'W': ('power', 'active'),\n",
    "    'active': ('power', 'active'),\n",
    "    'energy': ('energy', 'apparent'),\n",
    "    'A': ('current', ''),\n",
    "    'reactive_power': ('power', 'reactive'),\n",
    "    'apparent_power': ('power', 'apparent'),\n",
    "    'power_factor': ('pf', ''),\n",
    "    'PF': ('pf', ''),\n",
    "    'phase_angle': ('phi', ''),\n",
    "    'VA': ('power', 'apparent'),\n",
    "    'VAR': ('power', 'reactive'),\n",
    "    'reactive': ('power', 'reactive'),\n",
    "    'VLN': ('voltage', \"\"),\n",
    "    'V': ('voltage', \"\"),\n",
    "    'f': ('frequency', \"\")\n",
    "    \n",
    "}\n",
    "# data for file name manipulation\n",
    "TYPE_A = \"active\"\n",
    "TYPE_R = \"reactive\"\n",
    "\n",
    "filename_prefix_mapping = {\n",
    "    TYPE_A : ('4-POWER_REAL_FINE '),\n",
    "    TYPE_R : ('5-POWER_REACTIVE_STANDARD ')\n",
    "}\n",
    "filename_suffix_mapping = {\n",
    "    TYPE_A : (' Dump'),\n",
    "    TYPE_R : (' Dump')\n",
    "}\n",
    "\n",
    "# DataFrame column names\n",
    "TIMESTAMP_COLUMN_NAME = \"timestamp\"\n",
    "ACTIVE_COLUMN_NAME = \"VA\"\n",
    "REACTIVE_COLUMN_NAME = \"reactive\"\n",
    "\n",
    "type_column_mapping = {\n",
    "    TYPE_A : (ACTIVE_COLUMN_NAME),\n",
    "    TYPE_R : (REACTIVE_COLUMN_NAME) \n",
    "}\n",
    "\n",
    "\n",
    "TIMEZONE = \"Europe/London\" # local time zone\n",
    "home_dir='/Users/GJWood/nilm_gjw_data' # path to input data\n",
    "\n",
    "#regular expression matching\n",
    "bld_re = re.compile('building\\d+') #used to pull building name from directory path\n",
    "bld_nbr_re = re.compile ('\\d+') # used to pull the building number from the name\n",
    "iso_date_re = re.compile ('\\d{4}-\\d{2}-\\d{2}') # used to pull the date from the file name\n",
    "\n",
    "def convert_gjw(gjw_path, output_filename, format=\"HDF\"):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    gjw_path : str\n",
    "        The root path of the gjw dataset.\n",
    "    output_filename : str\n",
    "        The destination filename (including path and suffix), will default if not specified\n",
    "    directory and file structure\n",
    "    nilm_gjw_data\n",
    "        building<1>\n",
    "            elec\n",
    "                4-POWER_REAL_FINE <date> Dump.csv\n",
    "                5-POWER_REACTIVE_STANDARD <date> Dump.csv\n",
    "                ...\n",
    "        ...\n",
    "        building<n>\n",
    "        HDF5\n",
    "            nilm_gjw_data.hdf5\n",
    "        metadata\n",
    "            building1.yaml\n",
    "            dataset.yaml\n",
    "            meter_devices.yaml\n",
    "        other files    \n",
    "    \"\"\"\n",
    "    if gjw_path is None: gjw_path = home_dir\n",
    "    check_directory_exists(gjw_path)\n",
    "    os.chdir(gjw_path)\n",
    "    gjw_path = os.getcwd()  # sort out potential issue with slashes or backslashes\n",
    "    if output_filename is None:\n",
    "        output_filename =join(home_dir,'HDF5','nilm_gjw_data.hdf5')\n",
    "    # Open data store\n",
    "    print( 'opening datastore', output_filename)\n",
    "    store = get_datastore(output_filename, format, mode='w')\n",
    "    # walk the directory tree from the dataset home directory\n",
    "    #clear dataframe & add column headers\n",
    "    df = pd.DataFrame(columns=[ACTIVE_COLUMN_NAME,REACTIVE_COLUMN_NAME])\n",
    "    found = False\n",
    "    for current_dir, dirs_in_current_dir, files in os.walk(gjw_path):\n",
    "        if current_dir.find('.git')!=-1 or current_dir.find('.ipynb') != -1:\n",
    "            #print( 'Skipping ', current_dir)\n",
    "            continue\n",
    "        print( 'checking', current_dir)\n",
    "        m = bld_re.search(current_dir)\n",
    "        if m: #The csv files may be further down the tree so this section may be repeated\n",
    "            building_name = m.group()\n",
    "            building_nbr = int(bld_nbr_re.search(building_name).group())\n",
    "            meter_nbr = 1\n",
    "            key = Key(building=building_nbr, meter=meter_nbr)\n",
    "        for items in fnmatch.filter(files, \"4*.csv\"):\n",
    "            # process any .CSV files found\n",
    "            found = True\n",
    "            ds = iso_date_re.search(items).group()\n",
    "            print( 'found files for date:', ds,end=\" \")\n",
    "            # found files to process\n",
    "            df1 = _read_file_pair(current_dir,ds) # read two csv files into a dataframe    \n",
    "            df = pd.concat([df,df1]) # concatenate the results into one long dataframe\n",
    "        if found:\n",
    "            found = False\n",
    "            df = _prepare_data_for_toolkit(df)\n",
    "            _summarise_dataframe(df,'Prepared for tool kit')\n",
    "            store.put(str(key), df)\n",
    "            #clear dataframe & add column headers\n",
    "            #df = pd.DataFrame(columns=[ACTIVE_COLUMN_NAME,REACTIVE_COLUMN_NAME])\n",
    "            break # only 1 folder with .csv files at present\n",
    "    store.close()\n",
    "    convert_yaml_to_hdf5(join(gjw_path, 'metadata'),output_filename)\n",
    "    print(\"Done converting gjw to HDF5!\")\n",
    "\n",
    "def _read_and_standardise_file(cdir,ds,mtype):   \n",
    "    \"\"\"\n",
    "    parameters \n",
    "        cdir  - the directory path where the files may be found\n",
    "        ds   - the date string which identifies the pair of files\n",
    "        type - the type of data to be read\n",
    "    The filename is constructed using the appropriate prefixes and suffixes\n",
    "    The data is then read, merged, de-duplicated, converted to the correct time zone\n",
    "    and converted to a time series and resampled per second\n",
    "    \"\"\"\n",
    "    fn = filename_prefix_mapping[mtype]+ds+filename_suffix_mapping[mtype]+'.csv'\n",
    "    ffn = join(cdir,fn)\n",
    "    df = pd.read_csv(ffn,names=[TIMESTAMP_COLUMN_NAME,type_column_mapping[mtype]])\n",
    "    df.drop_duplicates(subset=[TIMESTAMP_COLUMN_NAME], inplace=True) # remove duplicate rows with same timestamp\n",
    "    df.index = pd.to_datetime(df.timestamp.values, unit='s', utc=True) # convert the index to time based\n",
    "    df = df.tz_convert(TIMEZONE) #deal with summertime etc. for London timezone\n",
    "    # re-sample on single file only as there may be gaps between dumps            \n",
    "    df = df.resample('S',fill_method='ffill') # make sure we have a reading for every second\n",
    "    # resample seems to remove the timestamp column so put it back\n",
    "    df[TIMESTAMP_COLUMN_NAME] = df.index\n",
    "    df.drop_duplicates(subset=TIMESTAMP_COLUMN_NAME, inplace=True)\n",
    "    return df\n",
    "\n",
    "def _read_file_pair(cdir,ds):\n",
    "    \"\"\"\"\n",
    "    parameters \n",
    "        cdir - the directory path where the files may be found\n",
    "        ds  - the date string which identifies the pair of files\n",
    "    The files are processed individually then the columns merged on matching timestamps   \n",
    "    \"\"\"\n",
    "    df1 = _read_and_standardise_file(cdir,ds,TYPE_A)\n",
    "    #_summarise_dataframe(df1,'read file: '+TYPE_A)\n",
    "    df2 = _read_and_standardise_file(cdir,ds,TYPE_R)\n",
    "    #_summarise_dataframe(df2,'read file: '+TYPE_R)  \n",
    "    df3 = pd.merge(df1,df2,on=TIMESTAMP_COLUMN_NAME, how='outer') #merge the two column types into 1 frame\n",
    "    df3.fillna(value=0, inplace=True) # may need to enter initial entries to reactive sequence\n",
    "    #_summarise_dataframe(df3,'return from merge and fillna)\n",
    "    print(df3[TIMESTAMP_COLUMN_NAME].head(1),\"to\",df3[TIMESTAMP_COLUMN_NAME].tail(1)) #print first and last entries\n",
    "    return df3\n",
    "\n",
    "def _prepare_data_for_toolkit(df):\n",
    "    #remove any duplicate timestamps between files\n",
    "    df.drop_duplicates(subset=[\"timestamp\"], inplace=True) # remove duplicate rows with same timestamp\n",
    "    df.index = pd.to_datetime(df.timestamp.values, unit='s', utc=True) # convert the index to time based\n",
    "    df = df.tz_convert(TIMEZONE) #deal with summertime etc. for London timezone\n",
    "    df = df.drop(TIMESTAMP_COLUMN_NAME,1) # remove the timestamp column  \n",
    "    df.rename(columns=lambda x: column_mapping[x], inplace=True) # Renaming from gjw header to nilmtk controlled vocabulary\n",
    "    df.columns.set_names(LEVEL_NAMES, inplace=True) # Needed for column levelling (all converter need this line)\n",
    "    df = df.convert_objects(convert_numeric=True) # make sure everything is numeric\n",
    "    df = df.dropna() # drop rows with empty cells\n",
    "    df = df.astype(np.float32) # Change float 64 (default) to float 32 \n",
    "    df = df.sort_index() # Ensure that time series index is sorted\n",
    "    return df\n",
    "\n",
    "def _summarise_dataframe(df,loc):\n",
    "    print(df.head(4))\n",
    "    print(\"...\", len(df.index),\"rows at\", loc)\n",
    "    print (df.tail(4))\n",
    "    \n",
    "def main():\n",
    "    convert_gjw('c:/Users/GJWood/nilm_gjw_data', None)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
